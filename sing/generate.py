# -*- coding: utf-8 -*-
# Copyright (c) 2018-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import argparse
from pathlib import Path
import sys

from scipy.io import wavfile
import torch
from torch import nn
from torch.utils.data import DataLoader
import tqdm

from . import dsp, nsynth
from .fondation.batch import collate
from .fondation.datasets import DatasetSubset
from .fondation import utils
from .sequence.models import download_pretrained_model


def get_parser():
    parser = argparse.ArgumentParser(
        "sing.generate",
        description="Generate audio samples from a trained SING model",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument(
        "--model",
        type=Path,
        default="models/sing.th",
        help="Path to the trained SING model as outputted by sing.main")
    parser.add_argument(
        "--dl",
        action="store_true",
        help="Download if necessary a pretrained SING model.")
    parser.add_argument(
        "--output",
        type=Path,
        default="generated",
        help="Path where the generated samples will be saved")
    parser.add_argument(
        "--metadata",
        default=nsynth.get_metadata_path(),
        type=Path,
        help="path to the dataset metadata file")

    parser.add_argument(
        "list",
        type=Path,
        help="File containing a list of names from the nsynth dataset. "
        "Those notes will be generated by SING")
    parser.add_argument(
        "--batch-size", type=int, default=32, help="Batch size")
    parser.add_argument("--cuda", action="store_true", help="Use cuda")
    parser.add_argument(
        "--parallel", action="store_true", help="Use multiple gpus")
    parser.add_argument(
        "--unpad",
        default=512,
        type=int,
        help="Amount of unpadding to perform")
    return parser


def main():
    args = get_parser().parse_args()

    if not args.model.exists():
        if args.dl:
            print("Downloading pretrained SING model")
            args.model.parent.mkdir(parents=True, exist_ok=True)
            download_pretrained_model(args.model)
        else:
            utils.fatal("No model found for path {}. To download "
                        "a pretrained model, use --dl".format(args.model))
    elif args.dl:
        print(
            "WARNING: --dl is set but {} already exist.".format(args.model),
            file=sys.stderr)
    model = torch.load(args.model)

    if args.cuda:
        model.cuda()
    if args.parallel:
        model = nn.DataParallel(model)

    args.output.mkdir(exist_ok=True, parents=True)
    dataset = nsynth.NSynthMetadata(args.metadata)

    names = [name.strip() for name in open(args.list)]
    indexes = [dataset.names.index(name) for name in names]

    to_generate = DatasetSubset(dataset, indexes)
    loader = DataLoader(
        to_generate, batch_size=args.batch_size, collate_fn=collate)

    with tqdm.tqdm(total=len(to_generate), unit="ex") as bar:
        for batch in loader:
            if args.cuda:
                batch.cuda_()
            with torch.no_grad():
                rebuilt = model.forward(**batch.tensors)
                rebuilt = utils.unpad1d(rebuilt, args.unpad)
            for metadata, wav in zip(batch.metadata, rebuilt):
                path = args.output / (metadata['name'] + ".wav")
                wavfile.write(
                    str(path), metadata['sample_rate'],
                    dsp.float_wav_to_short(wav).cpu().detach().numpy())
            bar.update(len(batch))


if __name__ == "__main__":
    main()
